{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS) Tagging Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Part-of-Speech (POS) tagging is the task of assigning a grammatical category (such as noun, verb, adjective, etc.) to each word in a sentence. POS tags provide essential syntactic information that is useful in many natural language processing (NLP) applications, such as:\n",
    "\n",
    "- Text parsing and syntactic analysis\n",
    "- Named entity recognition\n",
    "- Machine translation\n",
    "- Information extraction\n",
    "\n",
    "In this project, we use the **Penn Treebank (PTB) corpus**, which is a well-known annotated dataset containing English sentences where each word is labeled with its correct POS tag.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "The main goal of this project is to **predict the POS tags for words in a sentence** using a statistical approach. We aim to:\n",
    "\n",
    "1. Build a **vocabulary** of words from the training dataset.\n",
    "2. Handle **unknown words** using morphological rules and special tokens.\n",
    "3. Construct **transition** and **emission probability matrices** from the training data.\n",
    "4. Implement the **Viterbi algorithm** to find the most likely sequence of POS tags for a given sentence.\n",
    "5. Evaluate the performance using **accuracy** on a test set.\n",
    "\n",
    "---\n",
    "\n",
    "## Approach\n",
    "\n",
    "### 1. Data Preparation\n",
    "\n",
    "- Load the **Penn Treebank** corpus from NLTK.\n",
    "- Split the corpus into **training** (90%) and **testing** (10%) sets.\n",
    "- Create a **vocabulary dictionary** mapping each word to a unique integer ID.\n",
    "- Preprocess the test corpus to replace unknown words with special tokens such as `--unk--`, `--unk_digit--`, or `--unk_noun--` based on simple morphological rules.\n",
    "\n",
    "### 2. Handling Unknown Words\n",
    "\n",
    "Unknown words are replaced using rules based on:\n",
    "\n",
    "- Digits → `--unk_digit--`\n",
    "- Uppercase letters → `--unk_upper--`\n",
    "- Punctuation → `--unk_punct--`\n",
    "- Common suffixes for nouns, verbs, adjectives, and adverbs → respective unknown tokens\n",
    "\n",
    "This helps the model generalize better to words not seen during training.\n",
    "\n",
    "### 3. Statistical Model\n",
    "\n",
    "We use a **Hidden Markov Model (HMM)** approach:\n",
    "\n",
    "- **Transition probabilities (A)**: Probability of tag_j given tag_i, i.e., \\( P(t_j | t_i) \\)\n",
    "- **Emission probabilities (B)**: Probability of word given tag, i.e., \\( P(w | t) \\)\n",
    "\n",
    "These are calculated from the training corpus with **additive smoothing** to avoid zero probabilities.\n",
    "\n",
    "### 4. Viterbi Algorithm\n",
    "\n",
    "The **Viterbi algorithm** is used to find the most likely sequence of POS tags for a given sequence of words. Steps:\n",
    "\n",
    "1. **Initialization**: Compute probabilities for the first word using the start state and emission probabilities.\n",
    "2. **Forward recursion**: For each word, compute the maximum probability of reaching each POS tag considering all possible previous tags.\n",
    "3. **Backtracking**: Recover the best sequence of POS tags using the stored backpointers.\n",
    "\n",
    "We implement the Viterbi algorithm using **log probabilities** to prevent numerical underflow caused by multiplying many small probabilities.\n",
    "\n",
    "### 5. Evaluation\n",
    "\n",
    "The predicted tags are compared against the ground truth in the test corpus. Accuracy is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct tags}}{\\text{Total number of words}}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb4020c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Data Loading, Vocabulary Building, and Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "## Data Loading\n",
    "\n",
    "The first step is to get the **Treebank corpus**, which is a collection of English sentences where each word is already labeled with its part-of-speech (POS) tag. We first check if the dataset is available, and if not, we download it. This corpus gives us sentences with correct word-tag pairs, which we will use to train and test our POS tagging model.\n",
    "\n",
    "We also make sure that the tools needed to split text into sentences and words are ready. This helps us prepare the data in a format suitable for further steps like building vocabulary and training the model.\n",
    "\n",
    "---\n",
    "\n",
    "## Vocabulary Building\n",
    "\n",
    "Once the data is loaded, the next step is to make a **vocabulary**, which is a list of all unique words in the dataset. This vocabulary is important because:\n",
    "\n",
    "- It helps the model recognize words it has seen before.\n",
    "- It lets us assign a number to each word so the model can work with them in calculations.\n",
    "- It helps us deal with words that are not in the training data.\n",
    "\n",
    "Having a clear and complete vocabulary ensures the model can learn the correct relationship between words and their POS tags.\n",
    "\n",
    "---\n",
    "\n",
    "## Handling Unknown Words\n",
    "\n",
    "In real text, we often see words that are not in our training dataset. These are called **unknown words**. To handle them, we assign special labels based on simple rules. For example, we check if a word has a number, punctuation, capital letters, or certain endings that hint at its type (like nouns or verbs). These special labels allow the model to guess the POS tag for unknown words more accurately.\n",
    "\n",
    "---\n",
    "\n",
    "## Preprocessing the Corpus\n",
    "\n",
    "Before using the data for training, we need to **preprocess** it. Preprocessing does a few things:\n",
    "\n",
    "1. Turns all sentences into a single sequence of words for easier processing.\n",
    "2. Replaces unknown words with their special labels from the previous step.\n",
    "3. Marks the end of each sentence to help the model understand sentence boundaries.\n",
    "\n",
    "This step makes sure the data is organized in a way the model can use, and prepares it for probabilistic methods like Hidden Markov Models (HMM) and algorithms like Viterbi.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Splitting\n",
    "\n",
    "To check how well our model works, we split the dataset into **training** and **test** sets. Most of the data is used for training so the model can learn patterns between words and POS tags. The remaining smaller portion is used for testing, which lets us see how well the model predicts tags on words it has not seen before. This ensures the results reflect the model’s true ability to generalize, not just memorization of the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for 'treebank' corpus...\n",
      "'treebank' corpus found.\n",
      "Number of sentences in treebank: 3914\n",
      "\n",
      "Sample tagged sentences:\n",
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], [('Rudolph', 'NNP'), ('Agnew', 'NNP'), (',', ','), ('55', 'CD'), ('years', 'NNS'), ('old', 'JJ'), ('and', 'CC'), ('former', 'JJ'), ('chairman', 'NN'), ('of', 'IN'), ('Consolidated', 'NNP'), ('Gold', 'NNP'), ('Fields', 'NNP'), ('PLC', 'NNP'), (',', ','), ('was', 'VBD'), ('named', 'VBN'), ('*-1', '-NONE-'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('of', 'IN'), ('this', 'DT'), ('British', 'JJ'), ('industrial', 'JJ'), ('conglomerate', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(\"Checking for 'treebank' corpus...\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/treebank')\n",
    "    print(\"'treebank' corpus found.\")\n",
    "except LookupError:\n",
    "    print(\"'treebank' corpus not found. Downloading...\")\n",
    "    nltk.download('treebank')\n",
    "    print(\"'treebank' corpus downloaded successfully.\")\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "tagged_sentences = treebank.tagged_sents()\n",
    "\n",
    "print(f\"Number of sentences in treebank: {len(tagged_sentences)}\")\n",
    "\n",
    "# Print a few sample sentences\n",
    "print(\"\\nSample tagged sentences:\")\n",
    "print(tagged_sentences[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for 'punkt' tokenizer...\n",
      "'punkt' tokenizer found.\n",
      "\n",
      "Loading Penn Treebank tagged sentences...\n",
      "Total sentences in corpus: 3914\n",
      "\n",
      "Example of a tagged sentence:\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "\n",
      "Vocabulary size: 12408\n",
      "\n",
      "First 50 words in vocabulary:\n",
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'30s\", \"'40s\", \"'50s\", \"'80s\", \"'82\", \"'86\", \"'S\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '*', '*-1', '*-10', '*-100', '*-101', '*-102', '*-103', '*-104', '*-105', '*-106', '*-107', '*-108', '*-109', '*-11', '*-110', '*-111', '*-112', '*-113', '*-114', '*-115', '*-116', '*-117', '*-118', '*-119', '*-12', '*-120', '*-121', '*-122', '*-123', '*-124']\n",
      "\n",
      "Last 50 words in vocabulary:\n",
      "['wrath', 'wrecking', 'wrenching', 'wrestling', 'wrists', 'write', 'write-downs', 'write-off', 'writer', 'writers', 'writing', 'written', 'wrong', 'wrongdoing', 'wrote', 'year', 'year-ago', 'year-earlier', 'year-end', 'year-long', 'year-to-year', 'yearly', 'years', 'yellow', 'yen', 'yen-denominated', 'yen-support', 'yes', 'yesterday', 'yet', 'yet-to-be-formed', 'yield', 'yielded', 'yielding', 'yields', 'yon', 'you', 'young', 'younger', 'youngsters', 'your', 'yourself', 'youth', 'youthful', 'yttrium-containing', 'zero', 'zinc', 'zip', 'zone', 'zoomed']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "# 1. Check/download punkt tokenizer\n",
    "print(\"Checking for 'punkt' tokenizer...\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"'punkt' tokenizer found.\")\n",
    "except LookupError:\n",
    "    print(\"'punkt' tokenizer not found. Downloading...\")\n",
    "    nltk.download('punkt')\n",
    "    print(\"'punkt' tokenizer downloaded successfully.\")\n",
    "\n",
    "# 2. Load Penn Treebank tagged sentences\n",
    "print(\"\\nLoading Penn Treebank tagged sentences...\")\n",
    "corpus = treebank.tagged_sents()\n",
    "print(f\"Total sentences in corpus: {len(corpus)}\")\n",
    "\n",
    "print(\"\\nExample of a tagged sentence:\")\n",
    "print(corpus[0])\n",
    "\n",
    "# 3. Build vocabulary from the corpus\n",
    "vocab = sorted({word for sent in corpus for (word, tag) in sent})\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(vocab)}\")\n",
    "\n",
    "print(\"\\nFirst 50 words in vocabulary:\")\n",
    "print(vocab[:50])\n",
    "\n",
    "print(\"\\nLast 50 words in vocabulary:\")\n",
    "print(vocab[-50:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary dictionary: word → unique integer ID\n",
      "!: 0\n",
      "#: 1\n",
      "$: 2\n",
      "%: 3\n",
      "&: 4\n",
      "': 5\n",
      "'': 6\n",
      "'30s: 7\n",
      "'40s: 8\n",
      "'50s: 9\n",
      "'80s: 10\n",
      "'82: 11\n",
      "'86: 12\n",
      "'S: 13\n",
      "'d: 14\n",
      "'ll: 15\n",
      "'m: 16\n",
      "'re: 17\n",
      "'s: 18\n",
      "'ve: 19\n",
      "*: 20\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary from Treebank corpus\n",
    "# A sorted list of unique words\n",
    "vocab_list = sorted({word for sent in corpus for (word, tag) in sent})\n",
    "\n",
    "# Create vocab dictionary: word → unique index\n",
    "vocab = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "\n",
    "print(\"Vocabulary dictionary: word → unique integer ID\")\n",
    "\n",
    "# Print first 20 entries\n",
    "cnt = 0\n",
    "for word, idx in vocab.items():\n",
    "    print(f\"{word}: {idx}\")\n",
    "    cnt += 1\n",
    "    if cnt > 20:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of the test corpus:\n",
      "[[('Kalamazoo', 'NNP'), (',', ','), ('Mich.-based', 'JJ'), ('First', 'NNP'), ('of', 'IN'), ('America', 'NNP'), ('said', 'VBD'), ('0', '-NONE-'), ('it', 'PRP'), ('will', 'MD'), ('eliminate', 'VB'), ('the', 'DT'), ('13', 'CD'), ('management', 'NN'), ('positions', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('former', 'JJ'), ('Midwest', 'NNP'), ('Financial', 'NNP'), ('parent', 'NN'), ('company', 'NN'), ('.', '.')], [('First', 'NNP'), ('of', 'IN'), ('America', 'NNP'), ('said', 'VBD'), ('0', '-NONE-'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('managers', 'NNS'), ('will', 'MD'), ('take', 'VB'), ('other', 'JJ'), ('jobs', 'NNS'), ('with', 'IN'), ('First', 'NNP'), ('of', 'IN'), ('America', 'NNP'), ('.', '.')], [('But', 'CC'), ('it', 'PRP'), ('said', 'VBD'), ('that', 'IN'), ('severance', 'NN'), ('payments', 'NNS'), ('to', 'TO'), ('those', 'DT'), ('executives', 'NNS'), ('not', 'RB'), ('staying', 'VBG'), ('with', 'IN'), ('the', 'DT'), ('company', 'NN'), ('will', 'MD'), ('reduce', 'VB'), ('First', 'NNP'), ('of', 'IN'), ('America', 'NNP'), (\"'s\", 'POS'), ('operating', 'NN'), ('results', 'NNS'), ('for', 'IN'), ('1989', 'CD'), ('by', 'IN'), ('$', '$'), ('3', 'CD'), ('million', 'CD'), ('*U*', '-NONE-'), ('to', 'TO'), ('$', '$'), ('4', 'CD'), ('million', 'CD'), ('*U*', '-NONE-'), (',', ','), ('or', 'CC'), ('15', 'CD'), ('cents', 'NNS'), ('to', 'TO'), ('20', 'CD'), ('cents', 'NNS'), ('a', 'DT'), ('share', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "import string\n",
    "\n",
    "# Load the Penn Treebank test corpus\n",
    "sentences = treebank.tagged_sents()\n",
    "\n",
    "# Use last 10% as test corpus\n",
    "split_idx = int(len(sentences) * 0.9)\n",
    "test_corpus = sentences[split_idx:]   # list of [(word, tag), ...]\n",
    "\n",
    "print(\"A sample of the test corpus:\")\n",
    "print(test_corpus[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ba43762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation characters for unknown word handling\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "# Morphology suffix rules\n",
    "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\",\n",
    "               \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\",\n",
    "               \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "# Unknown word assignment function\n",
    "def assign_unk(tok):\n",
    "    \"\"\"\n",
    "    Assign unknown word tokens using morphological and lexical rules.\n",
    "    \"\"\"\n",
    "    # Digit-containing word\n",
    "    if any(char.isdigit() for char in tok):\n",
    "        return \"--unk_digit--\"\n",
    "\n",
    "    # Punctuation tokens\n",
    "    elif any(char in punct for char in tok):\n",
    "        return \"--unk_punct--\"\n",
    "\n",
    "    # Capitalized or upper-case word\n",
    "    elif any(char.isupper() for char in tok):\n",
    "        return \"--unk_upper--\"\n",
    "\n",
    "    # Likely nouns based on suffix\n",
    "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "\n",
    "    # Likely verbs based on suffix\n",
    "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "\n",
    "    # Likely adjectives based on suffix\n",
    "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "\n",
    "    # Likely adverbs based on suffix\n",
    "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "\n",
    "    # Default unknown token\n",
    "    return \"--unk--\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0951b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(vocab, sentences):\n",
    "    \"\"\"\n",
    "    Preprocess Treebank sentences:\n",
    "    - Flatten words\n",
    "    - Handle unknown words\n",
    "    - Replace sentence breaks with --n--\n",
    "    \"\"\"\n",
    "    orig = []\n",
    "    prep = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        for word, tag in sent:\n",
    "\n",
    "            # Save original\n",
    "            orig.append(word)\n",
    "\n",
    "            # Unknown word\n",
    "            if word not in vocab:\n",
    "                prep.append(assign_unk(word))\n",
    "            else:\n",
    "                prep.append(word)\n",
    "\n",
    "        # Add sentence boundary marker\n",
    "        orig.append(\"\")\n",
    "        prep.append(\"--n--\")\n",
    "\n",
    "    return orig, prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a186974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = sentences[:split_idx]\n",
    "test_sentences  = sentences[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the preprocessed test corpus: 10217\n",
      "This is a sample of the preprocessed test corpus:\n",
      "['Kalamazoo', ',', 'Mich.-based', 'First', 'of', 'America', 'said', '0', 'it', 'will']\n"
     ]
    }
   ],
   "source": [
    "# corpus without tags, preprocessed\n",
    "orig_test, prep = preprocess(vocab, test_sentences)\n",
    "\n",
    "print('The length of the preprocessed test corpus:', len(prep))\n",
    "print('This is a sample of the preprocessed test corpus:')\n",
    "print(prep[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7c64e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Creating Dictionaries and Baseline POS Prediction\n",
    "\n",
    "---\n",
    "\n",
    "After preprocessing, the next step is to prepare the information the model needs to learn how words are linked to POS tags and how tags follow each other in sentences. We do this by creating three types of counts from the training data:\n",
    "\n",
    "1. **Emission Counts**: Count how often a word appears with a particular tag. This tells the model the probability of a tag given a word.\n",
    "\n",
    "2. **Transition Counts**: Count how often one tag follows another. This helps the model understand the natural order of tags in sentences.\n",
    "\n",
    "3. **Tag Counts**: Count how often each tag appears in total. This is used to normalize probabilities and understand the overall tag distribution.\n",
    "\n",
    "We loop through the training sentences and update these counts for every word-tag pair. Unknown words are replaced by special tokens based on their type, like numbers, punctuation, capital letters, or common suffixes. We also mark the start and end of sentences so the model knows sentence boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "## Baseline POS Prediction\n",
    "\n",
    "With these counts ready, we build a simple baseline tagger. For each word, we choose the tag that occurs most often with it in the training data. We then compare these predictions with the true tags in the test data. Words are counted as correct if the predicted tag matches the true tag. Empty lines or invalid word-tag pairs are skipped. The accuracy is calculated as the percentage of correct predictions.\n",
    "\n",
    "This baseline gives a starting point by only looking at word-tag relationships, without considering tag sequences, and sets the stage for more advanced methods like HMM and Viterbi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bb388fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag(word, tag, vocab):\n",
    "    \"\"\"\n",
    "    Handle unknown words and return (word, tag)\n",
    "    \"\"\"\n",
    "    if word not in vocab:\n",
    "        word = assign_unk(word)\n",
    "\n",
    "    return word, tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_dictionaries(training_sentences, vocab, verbose=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        training_sentences: list of sentences from Treebank\n",
    "        vocab: dictionary of known words\n",
    "    Output:\n",
    "        emission_counts\n",
    "        transition_counts\n",
    "        tag_counts\n",
    "    \"\"\"\n",
    "\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "\n",
    "    # For transition model: start state\n",
    "    prev_tag = \"--s--\"\n",
    "\n",
    "    word_counter = 0\n",
    "\n",
    "    # Loop through each sentence\n",
    "    for sent in training_sentences:\n",
    "        for word, tag in sent:\n",
    "            word_counter += 1\n",
    "\n",
    "            if verbose and word_counter % 50000 == 0:\n",
    "                print(f\"word count = {word_counter}\")\n",
    "\n",
    "            # Handle unknown words\n",
    "            word, tag = get_word_tag(word, tag, vocab)\n",
    "\n",
    "            # TRANSITION: previous tag → current tag\n",
    "            transition_counts[(prev_tag, tag)] += 1\n",
    "\n",
    "            # EMISSION: tag emits word\n",
    "            emission_counts[(tag, word)] += 1\n",
    "\n",
    "            # Count tag frequency\n",
    "            tag_counts[tag] += 1\n",
    "\n",
    "            # Update prev tag\n",
    "            prev_tag = tag\n",
    "\n",
    "        # End of sentence\n",
    "        transition_counts[(prev_tag, \"--s--\")] += 1\n",
    "        prev_tag = \"--s--\"\n",
    "\n",
    "    return emission_counts, transition_counts, tag_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count = 50000\n"
     ]
    }
   ],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(train_sentences, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POS tags (number of 'states'): 46\n",
      "View these POS tags (states)\n",
      "['#', '$', \"''\", ',', '-LRB-', '-NONE-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "# get all the POS states\n",
    "states = sorted(tag_counts.keys())\n",
    "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
    "print(\"View these POS tags (states)\")\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'states' are the Parts-of-speech designations found in the training data. They will also be referred to as 'tags' or POS in this Notebook. \n",
    "\n",
    "- \"NN\" is noun, singular, \n",
    "- 'NNS' is noun, plural. \n",
    "- In addition, there are helpful tags like '--s--' which indicate a start of a sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition examples:\n",
      "(('--s--', 'NNP'), 681)\n",
      "(('NNP', 'NNP'), 3264)\n",
      "(('NNP', ','), 1335)\n",
      "((',', 'CD'), 108)\n",
      "(('CD', 'NNS'), 462)\n"
     ]
    }
   ],
   "source": [
    "print(\"transition examples:\")\n",
    "for ex in list(transition_counts.items())[:5]:\n",
    "    print(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad754432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "emission examples:\n",
      "(('CD', '9.8'), 2)\n",
      "(('CD', 'billion'), 125)\n",
      "(('VBN', 'sold'), 23)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nemission examples:\")\n",
    "for ex in list(emission_counts.items())[200:203]:\n",
    "    print(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16c5c381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ambiguous word example:\n",
      "('NN', 'back') 2\n",
      "('VB', 'back') 1\n",
      "('RB', 'back') 15\n",
      "('RP', 'back') 3\n",
      "('JJ', 'back') 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nambiguous word example:\")\n",
    "for tup, cnt in emission_counts.items():\n",
    "    if tup[1] == 'back':\n",
    "        print(tup, cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 - Testing\n",
    "\n",
    "Now we will test the accuracy of our parts-of-speech tagger using your `emission_counts` dictionary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcfffffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test sentences into \"word POS\" lines like WSJ_24.pos format\n",
    "y = []\n",
    "for sent in test_sentences:\n",
    "    for word, tag in sent:\n",
    "        y.append(f\"{word} {tag}\")\n",
    "    y.append(\"\")   # sentence boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    \"\"\"\n",
    "    Baseline POS tag predictor:\n",
    "    For each word, choose the POS tag with highest emission count.\n",
    "    \"\"\"\n",
    "\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for word, y_line in zip(prep, y):\n",
    "\n",
    "        # Skip empty sentence boundary\n",
    "        if y_line.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        parts = y_line.split()\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "\n",
    "        true_word, true_tag = parts\n",
    "\n",
    "        best_tag = None\n",
    "        best_count = 0\n",
    "\n",
    "        # Word is already preprocessed\n",
    "        # If unknown → already replaced in preprocess()\n",
    "        for tag in states:\n",
    "\n",
    "            key = (tag, word)\n",
    "\n",
    "            if key in emission_counts:\n",
    "                count = emission_counts[key]\n",
    "\n",
    "                if count > best_count:\n",
    "                    best_count = count\n",
    "                    best_tag = tag\n",
    "\n",
    "        # Compare prediction with true label\n",
    "        if best_tag == true_tag:\n",
    "            num_correct += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "    return num_correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline prediction accuracy: 86.49%\n"
     ]
    }
   ],
   "source": [
    "accuracy = predict_pos(prep, y, emission_counts, vocab, states)\n",
    "print(f\"Baseline prediction accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8727ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Creating Transition and Emission Matrices\n",
    "\n",
    "---\n",
    "\n",
    "After counting how often words appear with tags and how tags follow each other, the next step is to convert these counts into **probabilities**. These probabilities are what a **Hidden Markov Model (HMM)** uses to predict sequences of tags for sentences. We do this by creating two main matrices:\n",
    "\n",
    "1. **Transition Matrix (A)**:  \n",
    "   This matrix shows how likely it is for one tag to follow another. Each value `A[i, j]` represents the probability of tag `j` coming after tag `i`. These probabilities are calculated by dividing the number of times a tag pair occurs by the total occurrences of the first tag. To handle cases where some tag pairs do not appear in the training data, we add a small value to all entries, a technique called **smoothing**, which prevents zero probabilities.\n",
    "\n",
    "2. **Emission Matrix (B)**:  \n",
    "   This matrix tells how likely a word is to appear for a particular tag. Each entry `B[i, j]` is the probability that the `j`-th word is generated by the `i`-th tag. We calculate this by dividing the count of a word-tag pair by the total count of that tag. We also apply smoothing here to handle cases where some words may not have appeared with certain tags in the training data.\n",
    "\n",
    "---\n",
    "\n",
    "## Role of These Matrices in HMM\n",
    "\n",
    "* The **transition matrix** captures the structure of language at the tag level by modeling which tags are likely to follow others.  \n",
    "* The **emission matrix** connects tags to the words they usually generate.  \n",
    "* Together, these matrices form the core of a **Hidden Markov Model (HMM)**. In an HMM, the tags are **hidden states** that generate observable words. The transition and emission probabilities allow the model to compute the most likely sequence of tags for a sentence.\n",
    "\n",
    "Using these matrices, we can apply algorithms like **Viterbi** to find the sequence of tags that has the highest probability, taking into account both the likelihood of each word given a tag and the likelihood of each tag sequence. Even without Viterbi, these matrices can give a rough estimate of the HMM’s performance by assigning each word the most probable tag according to emission probabilities alone, though this ignores the sequence information captured by the transitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    \"\"\"\n",
    "    Create the transition probability matrix A[i,j] = P(tag_j | tag_i)\n",
    "    \"\"\"\n",
    "    states = sorted(tag_counts.keys())\n",
    "    num_tags = len(states)\n",
    "    A = np.zeros((num_tags, num_tags))\n",
    "\n",
    "    for i in range(num_tags):\n",
    "        prev_tag = states[i]\n",
    "        prev_count = tag_counts[prev_tag]\n",
    "\n",
    "        for j in range(num_tags):\n",
    "            curr_tag = states[j]\n",
    "\n",
    "            count = transition_counts.get((prev_tag, curr_tag), 0)\n",
    "\n",
    "            # Add smoothing\n",
    "            A[i, j] = (count + alpha) / (prev_count + alpha * num_tags)\n",
    "\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A sample:\n",
      "          CD        DT            EX            FW        IN\n",
      "CD  0.178739  0.001582  3.163510e-07  3.163510e-07  0.037013\n",
      "DT  0.023385  0.001360  1.359611e-07  1.360970e-04  0.010197\n",
      "EX  0.000012  0.000012  1.175834e-05  1.175834e-05  0.000012\n",
      "FW  0.000247  0.000247  2.471577e-04  2.471577e-04  0.000247\n",
      "IN  0.063624  0.317557  1.124207e-03  1.125219e-04  0.016974\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "\n",
    "print(\"Transition matrix A sample:\")\n",
    "A_sub = pd.DataFrame(\n",
    "    A[10:15, 10:15], \n",
    "    index=states[10:15], \n",
    "    columns=states[10:15]\n",
    ")\n",
    "print(A_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    \"\"\"\n",
    "    Create emission matrix B[tag_i, word_j] = P(word_j | tag_i)\n",
    "    vocab must be a LIST of words (not dict)\n",
    "    \"\"\"\n",
    "    states = sorted(tag_counts.keys())\n",
    "    num_tags = len(states)\n",
    "\n",
    "    # Convert vocab dict → list of words in sorted order\n",
    "    word_list = sorted(vocab.keys())\n",
    "    num_words = len(word_list)\n",
    "\n",
    "    B = np.zeros((num_tags, num_words))\n",
    "\n",
    "    for i in range(num_tags):\n",
    "        tag = states[i]\n",
    "        tag_count = tag_counts[tag]\n",
    "\n",
    "        for j in range(num_words):\n",
    "            word = word_list[j]\n",
    "\n",
    "            count = emission_counts.get((tag, word), 0)\n",
    "\n",
    "            B[i, j] = (count + alpha) / (tag_count + alpha * num_words)\n",
    "\n",
    "    return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cdc84eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emission matrix B sample:\n",
      "        engineers\n",
      "CD   3.151186e-07\n",
      "NN   8.521947e-08\n",
      "NNS  3.659200e-04\n",
      "VB   4.305876e-07\n",
      "RB   3.852959e-07\n",
      "RP   4.844773e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Emission matrix B sample:\")\n",
    "\n",
    "# 1. Create the emission matrix B\n",
    "alpha = 0.001\n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, vocab)\n",
    "\n",
    "# 2. Sample words\n",
    "sample_words = [\"725\", \"adroitly\", \"engineers\", \"promoted\", \"synergy\"]\n",
    "\n",
    "# Filter ONLY words that exist in vocab\n",
    "valid_words = [w for w in sample_words if w in vocab]\n",
    "\n",
    "# Map valid words → their vocab indices\n",
    "cols = [vocab[w] for w in valid_words]\n",
    "\n",
    "# Sample tags\n",
    "sample_tags = [\"CD\", \"NN\", \"NNS\", \"VB\", \"RB\", \"RP\"]\n",
    "rows = [states.index(tag) for tag in sample_tags]\n",
    "\n",
    "# 3. Build submatrix\n",
    "B_sub = pd.DataFrame(\n",
    "    B[np.ix_(rows, cols)],\n",
    "    index=sample_tags,\n",
    "    columns=valid_words\n",
    ")\n",
    "\n",
    "print(B_sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0f123",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Viterbi Algorithm – Initialization\n",
    "\n",
    "---\n",
    "\n",
    "In part-of-speech (POS) tagging using a Hidden Markov Model (HMM), the **Viterbi algorithm** is used to find the most likely sequence of tags for a sentence. Before calculating probabilities for the whole sentence, the algorithm needs an **initialization step** to set up the necessary structures.\n",
    "\n",
    "During initialization, we create two important matrices:\n",
    "\n",
    "1. **Probability Matrix (`best_probs`)**:  \n",
    "   This stores the highest probability of each tag at every position in the sentence. For the first word, the probability is calculated using the likelihood of starting with that tag (transition from a special start state) and the likelihood of that tag generating the observed word (emission probability).\n",
    "\n",
    "2. **Backpointer Matrix (`best_paths`)**:  \n",
    "   This keeps track of which previous tag gave the highest probability for the current tag. It is later used to reconstruct the most likely sequence of tags.\n",
    "\n",
    "The first word is special because it has no previous tag. To handle this, we use a **start state** that represents the beginning of a sentence. The probabilities from the start state to each possible tag are combined with the emission probabilities for the first word. If the first word is not in the training vocabulary, we replace it with a special **unknown word token**. This allows the model to handle words it has never seen before.\n",
    "\n",
    "Initialization is important because it sets a strong starting point for the forward pass. By correctly calculating probabilities for the first word and handling unknown words, the Viterbi algorithm can efficiently compute the most likely sequence of tags for the rest of the sentence.\n",
    "\n",
    "In short, initialization prepares the Viterbi algorithm to predict POS tags by combining the structure of language (transition probabilities) and word-tag information (emission probabilities), while ensuring the model can handle words not seen during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a728acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unknown tokens to vocab and emission matrix if not already\n",
    "unk_tokens = [\"--unk--\", \"--unk_digit--\", \"--unk_upper--\", \"--unk_punct--\",\n",
    "              \"--unk_noun--\", \"--unk_verb--\", \"--unk_adj--\", \"--unk_adv--\", \"--n--\"]\n",
    "\n",
    "for token in unk_tokens:\n",
    "    if token not in vocab:\n",
    "        idx = len(vocab)\n",
    "        vocab[token] = idx\n",
    "        # Expand B to add a new column for this token\n",
    "        B = np.hstack([B, np.full((B.shape[0], 1), 1e-8)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    \"\"\"\n",
    "    Viterbi initialization and forward pass.\n",
    "\n",
    "    Input:\n",
    "        states: list of POS tags\n",
    "        tag_counts: dict mapping tag -> count\n",
    "        A: Transition matrix (num_tags x num_tags)\n",
    "        B: Emission matrix (num_tags x vocab_size)\n",
    "        corpus: list of words to tag (preprocessed)\n",
    "        vocab: dict mapping word -> index\n",
    "\n",
    "    Output:\n",
    "        best_probs: np.array (num_tags x len(corpus)) of probabilities\n",
    "        best_paths: np.array (num_tags x len(corpus)) of backpointers\n",
    "    \"\"\"\n",
    "    \n",
    "    num_tags = len(states)\n",
    "    seq_len = len(corpus)\n",
    "    \n",
    "    best_probs = np.zeros((num_tags, seq_len))\n",
    "    best_paths = np.zeros((num_tags, seq_len), dtype=int)\n",
    "    \n",
    "    # First word initialization\n",
    "    word = corpus[0]\n",
    "    if word not in vocab:\n",
    "        word = assign_unk(word)\n",
    "    \n",
    "    for i, tag in enumerate(states):\n",
    "        # Assuming start state is \"--s--\"\n",
    "        start_idx = states.index(\"--s--\") if \"--s--\" in states else None\n",
    "        transition_prob = A[start_idx, i] if start_idx is not None else 1.0 / num_tags\n",
    "        emission_prob = B[i, vocab[word]] if word in vocab else 1e-8  # very small for unknowns\n",
    "        best_probs[i, 0] = transition_prob * emission_prob\n",
    "        best_paths[i, 0] = 0  # no previous tag at t=0\n",
    "    \n",
    "    # Forward pass for t = 1 to T-1\n",
    "    for t in range(1, seq_len):\n",
    "        word = corpus[t]\n",
    "        if word not in vocab:\n",
    "            word = assign_unk(word)\n",
    "        \n",
    "        for j, curr_tag in enumerate(states):\n",
    "            max_prob = -1\n",
    "            max_state = 0\n",
    "            for i, prev_tag in enumerate(states):\n",
    "                prob = best_probs[i, t-1] * A[i, j]\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_state = i\n",
    "            emission_prob = B[j, vocab[word]] if word in vocab else 1e-8\n",
    "            best_probs[j, t] = max_prob * emission_prob\n",
    "            best_paths[j, t] = max_state\n",
    "            \n",
    "    return best_probs, best_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,0]: 0.0000\n",
      "best_paths[2,3]: 2\n"
     ]
    }
   ],
   "source": [
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)\n",
    "\n",
    "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\")\n",
    "print(f\"best_paths[2,3]: {best_paths[2,3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ddf12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4.1: Viterbi Forward Pass\n",
    "\n",
    "---\n",
    "\n",
    "After initializing the model, the next step is the **Viterbi forward pass**, which helps us find the most likely sequence of POS tags for a sentence. In this step, we go through the sentence one word at a time and calculate probabilities for all possible tags.\n",
    "\n",
    "For each word, the algorithm looks at every possible current tag and finds which previous tag gives the highest probability of reaching the current tag. It uses two kinds of probabilities:\n",
    "\n",
    "* **Transition probability** – the chance of one tag coming after another tag.  \n",
    "* **Emission probability** – the chance of a tag generating the current word.\n",
    "\n",
    "The forward pass keeps track of two things:\n",
    "\n",
    "* A **probability matrix** that stores the highest probability for each tag at each word.  \n",
    "* A **backpointer matrix** that remembers which previous tag gave this highest probability.\n",
    "\n",
    "By using these two matrices, the model can later find the best sequence of tags for the sentence. To avoid very small probabilities becoming zero, we often work with **log probabilities**, which makes calculations safer.\n",
    "\n",
    "Special tokens are used for **unknown words** so that even words not seen in training can get a reasonable tag.\n",
    "\n",
    "In simple words, the Viterbi forward pass combines information about the **structure of language** (how tags follow each other) and **word-tag information** (which words belong to which tags) to figure out the most likely tag for every word in the sentence. The result prepares us to find the final best sequence of tags using the **backward pass**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42b88659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def viterbi_forward_log(A, B, corpus, vocab, states, verbose=False):\n",
    "    \"\"\"\n",
    "    Viterbi forward algorithm using log probabilities to prevent underflow.\n",
    "    \"\"\"\n",
    "    num_tags = len(states)\n",
    "    seq_len = len(corpus)\n",
    "    \n",
    "    best_probs = np.full((num_tags, seq_len), -np.inf)  # log(0) = -inf\n",
    "    best_paths = np.zeros((num_tags, seq_len), dtype=int)\n",
    "    \n",
    "    start_idx = states.index(\"--s--\") if \"--s--\" in states else None\n",
    "    \n",
    "    # Initialization\n",
    "    word = corpus[0]\n",
    "    if word not in vocab:\n",
    "        word = assign_unk(word)\n",
    "    \n",
    "    for i, tag in enumerate(states):\n",
    "        trans_prob = A[start_idx, i] if start_idx is not None else 1.0 / num_tags\n",
    "        emis_prob = B[i, vocab[word]] if word in vocab else 1e-8\n",
    "        best_probs[i, 0] = np.log(trans_prob) + np.log(emis_prob)\n",
    "        best_paths[i, 0] = 0\n",
    "    \n",
    "    # Forward recursion\n",
    "    for t in range(1, seq_len):\n",
    "        word = corpus[t]\n",
    "        if word not in vocab:\n",
    "            word = assign_unk(word)\n",
    "        \n",
    "        for j, curr_tag in enumerate(states):\n",
    "            max_prob = -np.inf\n",
    "            max_state = 0\n",
    "            for i, prev_tag in enumerate(states):\n",
    "                prob = best_probs[i, t-1] + np.log(A[i, j])\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_state = i\n",
    "            emis_prob = B[j, vocab[word]] if word in vocab else 1e-8\n",
    "            best_probs[j, t] = max_prob + np.log(emis_prob)\n",
    "            best_paths[j, t] = max_state\n",
    "        \n",
    "        if verbose and t % 100 == 0:\n",
    "            print(f\"Processed word {t}/{seq_len}: {corpus[t]}\")\n",
    "    \n",
    "    return best_probs, best_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,1]: -30.5394\n",
      "best_probs[0,4]: -65.3522\n"
     ]
    }
   ],
   "source": [
    "best_probs, best_paths = viterbi_forward_log(A, B, prep, vocab, states)\n",
    "\n",
    "print(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\")  # negative log-prob\n",
    "print(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53cbab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4.2: Viterbi Backward Pass\n",
    "\n",
    "---\n",
    "\n",
    "Once the forward pass has calculated the probabilities of all tags at each word, the **Viterbi backward pass** is used to find the actual sequence of tags for the sentence. This step is also called **backtracking**.\n",
    "\n",
    "The backward pass starts from the last word in the sentence. It looks at the **probability matrix** from the forward pass and picks the tag with the highest probability at the final word. This tag becomes the last tag in the predicted sequence.\n",
    "\n",
    "Then, using the **backpointer matrix** from the forward pass, the algorithm traces backwards through the sentence. For each word, it finds which previous tag led to the current tag with the highest probability. This way, it reconstructs the most likely sequence of tags from the last word back to the first word.\n",
    "\n",
    "After reaching the first word, the sequence is reversed to restore the correct order. Finally, the **tag indices** are converted back into their actual POS tag names, giving the final predicted sequence of tags for the entire sentence.\n",
    "\n",
    "In simple terms, the backward pass uses the information stored during the forward pass to assemble the **best possible tag sequence**. While the forward pass calculates probabilities, the backward pass actually tells us which tags to assign to each word. Together, the forward and backward passes make the Viterbi algorithm an effective method for POS tagging, combining the likelihood of **tag transitions** and **word-tag associations** to predict accurate sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1498908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_backward(best_probs, best_paths, states):\n",
    "    \"\"\"\n",
    "    Backtracking function for Viterbi algorithm.\n",
    "\n",
    "    Input:\n",
    "        best_probs: np.array of log-probabilities (num_tags x sequence length)\n",
    "        best_paths: np.array of backpointers (num_tags x sequence length)\n",
    "        states: list of POS tags corresponding to rows of best_probs\n",
    "\n",
    "    Output:\n",
    "        tag_sequence: list of predicted POS tags\n",
    "    \"\"\"\n",
    "    seq_len = best_probs.shape[1]\n",
    "\n",
    "    # Start from the tag with maximum probability at last position\n",
    "    last_tag_idx = np.argmax(best_probs[:, -1])\n",
    "    best_sequence = [last_tag_idx]\n",
    "\n",
    "    # Backtrack through the best_paths\n",
    "    for t in range(seq_len - 1, 0, -1):\n",
    "        last_tag_idx = best_paths[last_tag_idx, t]\n",
    "        best_sequence.append(last_tag_idx)\n",
    "\n",
    "    # Reverse the sequence to get correct order\n",
    "    best_sequence = best_sequence[::-1]\n",
    "\n",
    "    # Convert indices to tag names\n",
    "    tag_sequence = [states[i] for i in best_sequence]\n",
    "\n",
    "    return tag_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for the last 7 words is:\n",
      "Words: ['first', 'quarter', 'of', 'next', 'year', '.']\n",
      "Predicted tags: ['JJ', 'NN', 'IN', 'JJ', 'NN', '.'] \n",
      "\n",
      "The prediction for the first 7 words is:\n",
      "Words: ['Kalamazoo', ',', 'Mich.-based', 'First', 'of', 'America', 'said']\n",
      "Predicted tags: ['UH', ',', 'CC', 'NNP', 'IN', 'NNP', 'VBD']\n"
     ]
    }
   ],
   "source": [
    "pred = viterbi_backward(best_probs, best_paths, states)\n",
    "m = len(pred)\n",
    "\n",
    "print('The prediction for the last 7 words is:')\n",
    "print('Words:', prep[-7:m-1])\n",
    "print('Predicted tags:', pred[-7:m-1], \"\\n\")\n",
    "\n",
    "print('The prediction for the first 7 words is:')\n",
    "print('Words:', prep[0:7])\n",
    "print('Predicted tags:', pred[0:7])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Predicting and Evaluating POS Tags\n",
    "\n",
    "---\n",
    "\n",
    "After running the **Viterbi algorithm**, we predict the POS tag for each word in the test set. These predictions are compared with the **true tags** to compute **accuracy**, which measures the fraction of correctly predicted tags. Sentence boundaries and invalid lines are ignored. \n",
    "\n",
    "This evaluation shows how well the **Hidden Markov Model (HMM)** with Viterbi handles both **known and unknown words**, providing a clear measure of the model’s performance on **unseen data**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f35151c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(pred, y):\n",
    "    \"\"\"\n",
    "    Compute accuracy of predicted POS tags against ground truth.\n",
    "\n",
    "    Input: \n",
    "        pred: list of predicted POS tags (from Viterbi)\n",
    "        y: list of lines, each in \"word tag\" format (e.g., 'dog NN')\n",
    "    Output: \n",
    "        accuracy: fraction of correct predictions\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for prediction, y_line in zip(pred, y):\n",
    "        # Split the line into word and tag\n",
    "        word_tag = y_line.split()\n",
    "        \n",
    "        # Skip lines that don't have exactly 2 items (e.g., sentence boundaries)\n",
    "        if len(word_tag) != 2:\n",
    "            continue\n",
    "        \n",
    "        word, true_tag = word_tag\n",
    "        \n",
    "        # Compare predicted tag with true tag\n",
    "        if prediction == true_tag:\n",
    "            num_correct += 1\n",
    "        \n",
    "        total += 1\n",
    "\n",
    "    return num_correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8354290e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Viterbi algorithm is 90.24%\n"
     ]
    }
   ],
   "source": [
    "# pred is the list of POS tags predicted by viterbi_backward\n",
    "# y is the list of \"word tag\" lines from your test corpus\n",
    "\n",
    "accuracy = compute_accuracy(pred, y)\n",
    "print(f\"Accuracy of the Viterbi algorithm is {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa19fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd698b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e996b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9690ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
