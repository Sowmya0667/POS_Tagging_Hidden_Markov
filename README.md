# Parts of Speech Tagging Using Hidden Markov Models (HMM)

This repository contains an implementation of a **Part-of-Speech (POS) Tagger** built using a **Hidden Markov Model (HMM)** and the **Viterbi algorithm**. The project is designed to be simple, interpretable, and educational, focusing on how classical probabilistic models can be applied to Natural Language Processing tasks.

The work is based on the Penn Treebank corpus and demonstrates how transition probabilities, emission probabilities, and dynamic programming come together to solve POS tagging efficiently.

---

## ğŸ“Œ Project Overview

Part-of-Speech tagging assigns a grammatical label (noun, verb, adjective, etc.) to each word in a sentence. Since the same word can take different roles depending on context, sequence modeling is essential.

In this project:

* A **Hidden Markov Model (HMM)** is used to model tag sequences.
* **Transition probabilities** capture how likely one POS tag follows another.
* **Emission probabilities** capture how likely a word is generated by a given tag.
* The **Viterbi algorithm** is used to efficiently compute the most probable tag sequence for a sentence.
* Special handling is included for **unknown words**, improving robustness on real-world text.

---

## ğŸ§  Methodology

The POS tagging pipeline consists of two main phases:

### 1. Model Building

* Parse the annotated training corpus.
* Compute:

  * **Transition Matrix**: Probability of one tag following another.
  * **Emission Matrix**: Probability of a word given a tag.
* Estimate **initial tag probabilities** from sentence beginnings.

### 2. Tagging Using Viterbi Algorithm

* Apply dynamic programming to compute the most likely sequence of POS tags.
* Use log-probabilities to avoid numerical underflow.
* Apply smoothing to handle unseen transitions and emissions.

---

## ğŸ“‚ Dataset

* **Corpus**: Penn Treebank
* **Annotations**: Wordâ€“POS tag pairs using the Penn Treebank tagset

### Trainâ€“Test Split

* First 90% of sentences â†’ Training set
* Last 10% of sentences â†’ Test set

---

## ğŸ§¹ Data Preprocessing

### Unknown Word Handling

Words not seen during training are mapped to special categories based on:

* Digits
* Capitalization
* Punctuation
* Common suffixes (noun, verb, adjective, adverb)
* Default unknown token

This improves tagging accuracy for unseen or rare words.

### Sentence Boundary Handling

* Sentences are flattened into a single sequence.
* A special boundary token (`-n-`) marks sentence endings.

---

## ğŸ“Š Baseline POS Tagger

A simple baseline model is implemented for comparison:

* Assigns each word the POS tag with the **highest emission probability**.
* Ignores tag transitions and context.

**Baseline Accuracy**: **86.49%**

---

## ğŸ”— Hidden Markov Model (HMM)

### Components

* **Hidden States**: POS tags
* **Observations**: Words
* **Transition Matrix**: P(tagáµ¢ | tagáµ¢â‚‹â‚)
* **Emission Matrix**: P(word | tag)
* **Initial Probabilities**: Tag probabilities at sentence start

### Assumptions

* First-order Markov property
* Stationary transition probabilities
* Observations depend only on the current state

---

## ğŸš« Brute Force vs Viterbi

### Brute Force

* Tries all possible tag sequences
* Exponential time complexity (impractical)

### Viterbi Algorithm

* Dynamic programming approach
* Time complexity: **O(n Ã— kÂ²)**
* Efficient and optimal under HMM assumptions

---

## âš™ï¸ Viterbi Algorithm Steps

1. **Initialization** â€“ Set probabilities for the first word
2. **Recursion** â€“ Compute best probabilities for each word and tag
3. **Traceback** â€“ Recover the most probable tag sequence

Log-probabilities and smoothing are used for numerical stability.

---

## âœ… Results

* **HMM + Viterbi Accuracy**: **90.24%**
* Significant improvement over the baseline model

### Observations

* Contextual tag transitions greatly improve accuracy
* Unknown word handling increases robustness
* Dynamic programming makes the approach scalable

---

## ğŸ’ª Strengths

* Strong probabilistic foundation
* Efficient and interpretable
* Clear improvement over baseline
* Robust handling of unknown words

## âš ï¸ Limitations

* First-order Markov assumption
* Limited long-range dependency modeling
* Simple morphological rules for unknown words

## ğŸš€ Potential Improvements

* Higher-order HMMs
* Better smoothing techniques
* Character-level or embedding-based unknown word handling
* Hybrid models (CRF, BiLSTM-CRF)

---

## ğŸ“š References

* Jurafsky & Martin â€“ *Speech and Language Processing*
* Penn Treebank Corpus
* Analytics Vidhya â€“ POS Tagging & Viterbi Algorithm
* Research papers and lecture notes on HMM-based POS tagging

---

## ğŸ‘©â€ğŸ“ Author

**Boda Surya Venkata Jyothi Sowmya**
MSc Data Science
Chennai Mathematical Institute

---

â­ If you find this project helpful, feel free to star the repository!
